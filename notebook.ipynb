{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Load all markdown files from chapters directory\n",
    "loader = DirectoryLoader('./chapters', glob=\"**/*.md\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Add counter index to each document\n",
    "for i, doc in enumerate(documents):\n",
    "    doc.metadata['index'] = i\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Preserve the original document index in the chunks\n",
    "for text in texts:\n",
    "    text.metadata['doc_index'] = text.metadata['index']\n",
    "# Add chunk index and master index to each text\n",
    "master_index = 0\n",
    "for doc_index in range(max(text.metadata['doc_index'] for text in texts) + 1):\n",
    "    # Get all chunks for this document\n",
    "    doc_chunks = [t for t in texts if t.metadata['doc_index'] == doc_index]\n",
    "    \n",
    "    # Add chunk index within document\n",
    "    for chunk_idx, chunk in enumerate(doc_chunks):\n",
    "        chunk.metadata['chunk_index'] = chunk_idx\n",
    "        chunk.metadata['master_index'] = master_index\n",
    "        master_index += 1\n",
    "\n",
    "# Create embeddings and store in Chroma vector database\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=texts,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "# Create retriever interface\n",
    "retriever = vectordb.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 6}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chat interface with memory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Create system prompt for story assistant\n",
    "system_prompt = \"\"\"You are a knowledgeable assistant who helps answer questions about the story's history and lore. \n",
    "Use the provided context to give accurate and detailed responses about the story's world, characters, and events.\n",
    "If you're not sure about something, be honest about your uncertainty.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"\\nAsk me about the story (or type 'quit' to exit): \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "        \n",
    "    # Search relevant context\n",
    "    context_docs = retriever.get_relevant_documents(user_input)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in context_docs])\n",
    "    \n",
    "    # Create messages with context\n",
    "    response = prompt.format(\n",
    "        chat_history=memory.chat_memory.messages,\n",
    "        input=f\"Using this context:\\n{context}\\n\\nAnswer this question: {user_input}\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nAssistant:\", response)\n",
    "    \n",
    "    # Store conversation in memory\n",
    "    memory.chat_memory.add_user_message(user_input)\n",
    "    memory.chat_memory.add_ai_message(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chains for memory processing and story generation\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import io\n",
    "\n",
    "# Initialize LLMs with different temperatures for different creative needs\n",
    "experience_llm = ChatOpenAI(temperature=0.7)\n",
    "story_llm = ChatOpenAI(temperature=0.9)\n",
    "latex_llm = ChatOpenAI(temperature=0.3)\n",
    "\n",
    "# Create prompts for each stage\n",
    "experience_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\"],\n",
    "    template=\"\"\"Given these story elements and memories:\n",
    "\n",
    "{context}\n",
    "\n",
    "Suggest deeply human experiences, emotions, and universal themes that relate to these elements. \n",
    "Consider fundamental human experiences like love, loss, growth, fear, triumph, etc.\n",
    "Be specific and draw meaningful connections.\"\"\"\n",
    ")\n",
    "\n",
    "story_prompt = PromptTemplate(\n",
    "    input_variables=[\"experiences\", \"related_memories\"],\n",
    "    template=\"\"\"Using these human experiences and thematic elements:\n",
    "\n",
    "{experiences}\n",
    "\n",
    "And drawing from these related memories and story elements:\n",
    "\n",
    "{related_memories}\n",
    "\n",
    "Craft a rich and engaging story segment that weaves these elements together.\n",
    "Focus on vivid imagery, emotional resonance, and narrative flow.\"\"\"\n",
    ")\n",
    "\n",
    "latex_prompt = PromptTemplate(\n",
    "    input_variables=[\"story\"],\n",
    "    template=\"\"\"Convert this story segment into properly formatted LaTeX:\n",
    "\n",
    "{story}\n",
    "\n",
    "Use appropriate LaTeX formatting for literary text, including proper paragraph breaks,\n",
    "quotation marks, and any needed structural elements.\"\"\"\n",
    ")\n",
    "\n",
    "# Create the processing chains\n",
    "experience_chain = LLMChain(llm=experience_llm, prompt=experience_prompt)\n",
    "story_chain = LLMChain(llm=story_llm, prompt=story_prompt)\n",
    "latex_chain = LLMChain(llm=latex_llm, prompt=latex_prompt)\n",
    "\n",
    "# Create output stream\n",
    "output_file = io.StringIO()\n",
    "\n",
    "# Get all documents and their indices\n",
    "all_docs = retriever.get_relevant_documents(\"\")\n",
    "doc_indices = {doc.page_content: idx for idx, doc in enumerate(all_docs)}\n",
    "\n",
    "# Process each document chunk\n",
    "for current_idx, current_doc in enumerate(all_docs):\n",
    "    current_content = current_doc.page_content\n",
    "    \n",
    "    # Only retrieve documents that came before this one\n",
    "    related_docs = [doc for doc in all_docs \n",
    "                   if doc_indices[doc.page_content] < current_idx]\n",
    "    related_memories = \"\\n\\n\".join([doc.page_content for doc in related_docs])\n",
    "    \n",
    "    # Combine current content with related memories for context\n",
    "    full_context = f\"Current Memory:\\n{current_content}\\n\\nRelated Memories:\\n{related_memories}\"\n",
    "    \n",
    "    # Generate human experiences\n",
    "    experiences = experience_chain.run(context=full_context)\n",
    "    \n",
    "    # Generate story\n",
    "    story = story_chain.run(experiences=experiences, related_memories=related_memories)\n",
    "    \n",
    "    # Convert to LaTeX\n",
    "    latex_text = latex_chain.run(story=story)\n",
    "    \n",
    "    # Write to output stream\n",
    "    output_file.write(latex_text + \"\\n\\n\")\n",
    "\n",
    "# Get final output\n",
    "final_latex = output_file.getvalue()\n",
    "print(\"Generated LaTeX Story:\")\n",
    "print(final_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
